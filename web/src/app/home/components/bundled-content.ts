// This file is auto-generated from src/content/preview.mdx
// Do not edit manually - changes will be overwritten on build

export interface BundledPreviewData {
  frontmatter: Record<string, string>;
  content: string;
}

export const bundledPreviewContent: BundledPreviewData = {
  frontmatter: {
  "title": "AnotherAI: a MCP server designed for AI engineering",
  "description": "Public preview"
},
  content: "\nToday we're introducing a public preview of **AnotherAI**, a MCP server designed for AI engineering tasks that includes a set of tools that enables your AI assistant (such as Claude Code, Cursor, etc.) to:\n\n- run experiments to compare any models, and analyze the results (quality, speed, cost). [[docs](https://docs.anotherai.dev/use-cases/fundamentals/experiments)]\n- access production LLM completions to debug and improve agents based on real data. [[docs](https://docs.anotherai.dev/use-cases/fundamentals/debugging)]\n- collect and analyze users' feedback to improve an agent. [[docs](https://docs.anotherai.dev/use-cases/user-feedback)]\n- answer any questions about metrics (usage, performance, etc.) [[docs](https://docs.anotherai.dev/use-cases/fundamentals/metrics)]\n- deploy a new prompt or model without any code change [[docs](https://docs.anotherai.dev/use-cases/fundamentals/deployments)]\n\nOur work is available at:\n\n- https://anotherai.dev as a managed service, billed at the same cost as the underlying models (no markup).\n- https://github.com/anotherai-dev/anotherai under the Apache 2.0 license.\n\n## AI that can compare models' performance, price, and latency.\n\nAnotherAI's MCP server exposes tools that let your AI assistant access over 100 models, and compare their performance, price, and latency. In our own tests, we've found that models like Opus 4 are very good at reviewing work from other models, and the latest improvements in longer context windows (Sonnet and Gemini support up to 1M tokens) make it possible to compare more parameters (models and prompts) and agents with longer inputs.\n\n[video]\n\nSome prompt examples:\n\n```\n> can you compare Gemini 2.5 Flash, GPT-4o mini and Mistral Small for this agent \"<agent_name>\"?\n> can you find a model that is faster but keeps the same quality and does not cost more?\n> can you test how GPT-5 performs on this agent \"<agent_name>\"?\n> can you adjust the prompt for the agent \"<agent_name>\" to include few shot examples? validate that the outputs are improved.\n> ...\n```\n\nBecause your AI assistant can't always be trusted without a human in the loop, we've also implemented a web UI to review the experiments made by your AI assistant.\n\n[screenshot]\n\n## AI learns from production data.\n\nLearning from production usage is a key step to improving any AI agent. To learn from production usage, we have implemented an OpenAI compatible API that logs all the completions data coming through, and then our MCP server exposes these logs to your AI assistant.\n\nSome prompt examples:\n\n```\n> can you look at the last 20 completions for the agent \"<agent_name>\" and report back the ones that are not good?\n> can you understand why the customer \"<customer_email>\" had a bad experience with agent \"<agent_name>\"?\n> ...\n```\n\nLearn more about how to use the MCP server to learn from production data [here](https://docs.anotherai.dev/use-cases/fundamentals/debugging).\n\nSome people might not like the idea of adding a proxy as a new single point of failure in their LLM architecture, so we are also exploring exposing an API endpoint to import completions after they have been generated (like traditional observability tools). If you're interested in this feature, please get in touch on [Slack](https://join.slack.com/t/anotherai-dev/shared_invite/zt-3av2prezr-Lz10~8o~rSRQE72m_PyIJA) so we can design something that works well together.\n\n## AI learns from users' feedback.\n\nOn top of the completions logs, collecting users' feedback is another key step to improving any AI agent. To create a fluid feedback loop, we are exposing an Annotations API to let your end-users leave feedback on completions. Then our MCP server exposes these annotations to your AI assistant.\n\nWe believe that AI assistants are so good now that they are able to read users' feedback, identify issues, propose improvements, and run experiments to test changes using production data.\n\nSome prompt examples:\n\n```\n> can you look at the users' feedback for the agent \"<agent_name>\" in the last week, and write a report with the most common issues?\n> based on the users' feedback, think about some improvements we can make to the agent \"<agent_name>\" and run an experiment to test them using the latest production data.\n> ...\n```\n\nLearn more about how to use the MCP server to learn from users' feedback [here](https://docs.anotherai.dev/use-cases/user-feedback).\n\n## Deploy a new prompt or model without any code change.\n\nOne very popular feature of our previous product ([WorkflowAI](https://workflowai.com)) was the ability to update an agent's prompt or model without any code change. This feature enables faster iteration cycles, and fixing a prompt can be done without a PR and deployment. We've implemented the same feature in AnotherAI's MCP server, with a human confirmation step to prevent your AI assistant from making changes that are not intended.\n\nSome prompt examples:\n\n```\n> can you update the deployment \"<deployment_id>\" to use the model \"<model_name>\"?\n> update the prompt from \"<deployment_id>\" to use the prompt from this version \"<version_id>\"?\n> ...\n```\n\nLearn more about how to use the MCP server to deploy a new prompt or model without any code change [here](https://docs.anotherai.dev/use-cases/fundamentals/deployments).\n\n## AI deep dives into metrics.\n\nBecause our LLM gateway logs all the completions data, we wanted to give you and your AI assistant the best way to leverage this data. So we've designed two complementary components:\n\n- an MCP tool `query_completions(sql_query)` that allows your AI assistant to query the completions data using SQL queries. We've been really impressed by how good AI assistants are at transforming a natural language question into a complex SQL query. Using SQL instead of a predefined API allows the AI assistant to query the data in very powerful ways.\n- a web UI to view graphs and metrics about your agents. Your AI assistant can use the tool `create_or_update_view(view)` to create a view that will be saved and can be accessed in the web UI.\n\nSome prompt examples:\n\n```\n> what is our most expensive agent? can we run an experiment to find a cheaper model that keeps the same quality?\n> what is the p90, and p99 latency for the agent \"<agent_name>\"?\n> can you create a graph that shows the cost by agent in the last month?\n```\n\n[video]\n\nWe've also published a note about how we have secured the `query_completions` tool [here](https://docs.anotherai.dev/security#sql-query-tool-security) from malicious use. We welcome more feedback on our approach via our [Slack](https://join.slack.com/t/anotherai-dev/shared_invite/zt-3av2prezr-Lz10~8o~rSRQE72m_PyIJA)\n\n## Some (current) limitations.\n\nWe've focused this initial preview on simple AI agent architectures, not complex agentic systems. Agents that have multiple back-and-forth interactions or custom tools are harder to reproduce with other prompts and models because you need to be able to simulate one end of the conversation, and for custom tools you need to run the code somehow. If you're building a complex agentic system, please get in touch on [Slack](https://join.slack.com/t/anotherai-dev/shared_invite/zt-3av2prezr-Lz10~8o~rSRQE72m_PyIJA) so we can design something that works well together.\n\nFor very low latency agents, using AnotherAI's LLM gateway might not be the best option due to the added latency of the gateway, which we estimate at ~100ms. It's also possible to use AnotherAI's MCP server independently from the AI gateway to run experiments between models and prompts.\n\n## Try it\n\nThe first step is to install the MCP server, you can find the instructions [here](https://docs.anotherai.dev/getting-started). Once the MCP server is installed, find your first use-case by looking at our use-cases in the [docs](https://docs.anotherai.dev/). New accounts will get $1 of free credits to try it out.\n\nWe are really excited to hear from you, please join our [Slack channel](https://join.slack.com/t/anotherai-dev/shared_invite/zt-3av2prezr-Lz10~8o~rSRQE72m_PyIJA) to share what you're building with AnotherAI, meet our team, ask any questions, or just say hi.\n\nAnotherAI's team.\n[Pierre](https://x.com/pierrevalade), Anya, Guillaume, Jacek.\n\n## FAQ\n\n<Accordions>\n  <Accordion title=\"How does AnotherAI MCP gets access to the completions data?\">\n    AnotherAI MCP gets access to the completions data via the AnotherAI LLM gateway. The LLM gateway logs all the\n    completions data and makes it available to the MCP server.\n  </Accordion>\n</Accordions>\n",
};
