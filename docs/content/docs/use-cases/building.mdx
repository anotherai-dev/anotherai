---
title: Building a New Agent
summary: Learn how to build AI agents with AnotherAI's unified API and observability features
description: Step-by-step guide to creating, configuring, and deploying AI agents using AnotherAI THIS NEEDS TO BE UPDATED
---

import { Step, Steps } from 'fumadocs-ui/components/steps';

<Callout type="error">
This description (above) needs to be updated to reflect the new value proposition of using AnotherAI + Agentic coding tools for building agents.
</Callout>

## Getting Started

Because AnotherAI is an OpenAI-compatible proxy, building an agent has minimal configuration changes from standard OpenAI implementations. There are a few things you need to know to make sure your agent is AnotherAI-compatible.

### Recommended: Use Claude Code for Quick Agent Creation

<Callout type="error">
Should we completely assume that users will use a MCP client to build agents? And go all in on that?

We should keep the integrations documentation available with the code examples, but mostly for the MCP clients reading the documentation, not as much the end-user. 

Currently, on this page, most of the content is dedicated to the manual setup, which is not the recommended approach.
</Callout>

The easiest way to create a new agent is to ask Claude Code (or your preferred AI coding agent) to build it for you:

```
Create a new AnotherAI agent that analyzes customer sentiment from support tickets
```

```
Build an agent using AnotherAI that extracts structured data from invoices
```

<Callout type="error">
Do we currently support PDF as input well? I thought we had issues with images for example in the playground tool.
</Callout>

Claude Code will automatically set up the proper configuration and best practices for you.

[IMAGE TODO: PROMPT SENT TO CLAUDE CODE??]

<Callout type="error">
This page currently misses the core value proposition of using AnotherAI + Agentic coding tools for building agents.

**The Experimentation Flow**
When building a new agent with AnotherAI, the key value is automatic experimentation:
- Show how AnotherAI/Claude Code can automatically tests 100+ models (of course, we don't expect that 100 models will be tested, but at least the reader reading the page should understand that AnotherAI can test different models from different providers)
- Include screenshots of the experimentation page showing model/prompts side-by-side comparisons
- Comparing outputs quality, but also cost and latency that are automatically computed by AnotherAI
- Demonstrate how users can provide feebdack on outputs, and feedback loop to find the right models/prompts?

**Also, we shoud include an example of prompts that showcase how constraints can be specified during agent creation**
- How to specify performance requirements (max p50 latency)
- How to set budget constraints
- How AnotherAI automatically finds the best model within constraints
- Example prompts: "Create agent with less than 1s latency and under $0.xx for 1000 requests"
the goal is to show that AnotherAI can automatically find the best model within constraints, and that the user can just specify the constraints.
</Callout>

### Manual Setup

<Callout type="error">
- Should this section link to the integrations page instead? Since the code is actually different for each integration. 

I think too much of this page is dedicated to the manual setup, which is not the recommended approach.
</Callout>

If you prefer to build manually or want to understand the configuration details, follow the guide below.

<Steps>

<Step>

### Base URL and AnotherAI API Key Setup

AnotherAI provides a unified API that routes requests to various AI providers. To use AnotherAI instead of calling OpenAI directly, you need to:

1. **Change the base URL** - This redirects API calls from OpenAI's servers to AnotherAI, which then routes them to the appropriate provider while adding observability features
2. **Configure your AnotherAI API key** - This enables access to AnotherAI's features

<Tabs defaultValue="cloud-hosted" items={['Cloud-Hosted', 'Self-Hosted']}>

<Tab value="cloud-hosted">

For cloud-hosted AnotherAI, use your AnotherAI API key:

```python
import openai

client = openai.OpenAI(
    base_url="{{API_URL}}/v1",  # AnotherAI cloud endpoint
    api_key="aai-***",  # Your AnotherAI API key
)
```

</Tab>

<Tab value="self-hosted">

For self-hosted AnotherAI, point to your local instance with your AnotherAI API key:

```python
import openai

client = openai.OpenAI(
    base_url="http://localhost:8000/v1",  # Local AnotherAI instance
    api_key="aai-***",  # Your AnotherAI API key
)
```

</Tab>

</Tabs>

</Step>

<Step>

### Setting up Provider Keys

Provider keys determine which AI provider credentials AnotherAI uses to make requests to models.

**For self-hosted users**: Provider keys are required and must be configured via environment variables:

```bash
# .env file
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
GOOGLE_API_KEY=...
```

**For cloud users**: AnotherAI provides default provider access, but you can optionally bring your own keys for cost control or quota management through the web interface.

</Step>

<Step>

### Metadata

1. **Agent Identification**
In order to distinguish between different agents in AnotherAI's web view, include an `agent_id` in your agent's metadata.

```python
completion = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Analyze the sentiment of this product review"}],
    metadata={
        "agent_id": "product-review-sentiment",  # Required for observability
    }
)
```
2. **Workflow Identification**

If your agent is part of a workflow, it's recommended to include a `trace_id` and `workflow_name` in your metadata. You can read more about workflow set up [here](/use-cases/connecting-agents).

```python
completion = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Analyze the sentiment of this product review"}],
    metadata={
        "agent_id": "product-review-sentiment",
        "workflow_name": "review-analysis-pipeline",
        "trace_id": "trace-123e4567-e89b",  # Unique ID for this workflow instance
    }
)
```

</Step>

<Step>

### Input and Output Design

#### 1. Input Variables

If there is variable content in your prompts, use Jinja2 templates to separate static prompts from dynamic content:

```python
completion = client.chat.completions.create(
    model="claude-3-5-sonnet-20241022",
    messages=[{
        "role": "user", 
        "content": "Analyze the sentiment of this product review: {{review_text}}"
    }],
    extra_body={
        "input": {
            "review_text": "This product exceeded my expectations! The quality is amazing..."
        }
    },
    metadata={
        "agent_id": "product-review-sentiment",
        "workflow_name": "review-analysis-pipeline",
        "trace_id": "trace-123e4567-e89b",
    }
)
```

#### 2. Structured Outputs

Structured outputs aren't required, but they're highly recommended, especially for agents that have multiple output fields.

```python
from pydantic import BaseModel
from enum import Enum

class Sentiment(str, Enum):
    positive = "positive"
    negative = "negative"
    mixed = "mixed"

class SentimentAnalysis(BaseModel):
    sentiment: Sentiment
    explanation: str  # Why this sentiment was determined

completion = client.beta.chat.completions.parse(
    model="gpt-4o",
    messages=[{
        "role": "user", 
        "content": "Analyze the sentiment of this product review: {{review_text}}"
    }],
    response_format=SentimentAnalysis,
    extra_body={
        "input": {
            "review_text": "This product exceeded my expectations! The quality is amazing..."
        }
    },
    metadata={
        "agent_id": "product-review-sentiment",
        "workflow_name": "review-analysis-pipeline",
        "trace_id": "trace-123e4567-e89b",
    }
)

result = completion.choices[0].message.parsed
```

</Step>

</Steps>

### Next Steps

- [Test your agent with experiments](/docs/agents/evaluating)
- [Improve your agent with annotations](/docs/agents/improving)
- [Evaluate agent performance](/docs/agents/evaluating)
