---
title: Testing New Models on an Existing Agent
summary: Use case for evaluating new AI models against your production model
description: How to test and compare new models before switching
---

import { Steps, Step } from 'fumadocs-ui/components/steps';
import { Callout } from 'fumadocs-ui/components/callout';

Suppose you're currently using GPT-4o-mini for a given agent, but you've heard rave reviews about GPT-5 that was just released. You want to test whether the quality improvement justifies the higher cost before switching.

With how often new models are released, the above scenario is extremely common, so we wanted to make it as easy as possible for our users to test new models on existing agents.

### Creating a side by side model comparison
<Steps>
<Step>
Ask Claude to create a experiment between your current and new model

```
Compare how @calendar_event_extractor.py performs using current GPT-4o-mini 
vs the new GPT-5 model
```
</Step>
<Step>
If you want to be sure to test with real data, you can modify your prompt to include that instruction

```
Compare how @calendar_event_extractor.py performs using current GPT-4o-mini 
vs the new GPT-5 model. Use the inputs from the last 20 completions.
```

Alternatively, if you have a dataset of standard test inputs you like to use to validate changes, you can modify your prompt to include that instruction:

```
Compare how @calendar_event_extractor.py performs using current GPT-4o-mini 
vs the new GPT-5 model. Use the inputs from @email_test_cases.txt
```

[IMAGE TODO: SCREENSHOT OF EXPERIMENT CREATED FROM ONE OF THE ABOVE PROMPTS]
</Step>
</Steps>

<Callout type="info">
**Tip:** Use real production data for testing, not artificial examples to ensure you're testing the new model with real-life scenarios.
</Callout>
