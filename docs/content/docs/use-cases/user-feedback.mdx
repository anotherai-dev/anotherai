---
title: Using End-User Feedback to Improve Agents
summary: Guide for incorporating end-user feedback into agent development through annotations and feedback collection.
description: Collect and integrate end-user feedback to improve your agents
---

import { Steps, Step } from 'fumadocs-ui/components/steps';
import { Card, Cards } from 'fumadocs-ui/components/card';
import { FlaskRound, MessageSquare } from 'lucide-react';

# End-User Feedback

Incorporating end-user feedback into your agent's development process can be invaluable when creating effective agents. We recommend setting up user feedback in your product to be added directly to completions via annotations. 

Below is an example of the process you might implement for collecting and incorporating end-user feedback:

<Steps>
<Step>
### Set up user feedback collection in your product

This will look different for each product. Generally we recommend allowing the user to provide a comment, so that they can provide nuanced feedback. You may also want to allow your user to leave a score - for example 1-5 stars, or a thumbs up/down - in these cases the feedback would be added as a metric in the annotations.
</Step>

[IMAGE TODO: SCREENSHOT OF USER FEEDBACK COMPONENT IN M1 OR WORKFLOWAI???]

<Step>
### Write a script to send user feedback to AnotherAI via the annotations endpoint 


```python
# Example: Send user feedback as annotations via API
import requests
from datetime import datetime

# Configure your AnotherAI API endpoint and key
API_BASE_URL = "{{API_URL}}"
API_KEY = "your-api-key"

def submit_user_feedback(completion_id: str, rating: int, comment: str, user_id: str = "end_user"):
    """Submit user feedback as an annotation to AnotherAI"""
    
    annotation = {
        "id": f"feedback_{completion_id}_{datetime.now().timestamp()}",
        "target": {
            "completion_id": completion_id,
            # key_path can be used to annotate specific fields
            "key_path": None  # Annotating the entire completion
        },
        "author_name": user_id,
        "text": comment,
        "metric": {
            "name": "accuracy",
            "value": rating  # e.g., 1-5 star rating
        },
        "metadata": {
            "source": "feedback_widget",
            "additionalProp1": {}
        }
    }
    
    response = requests.post(
        f"{API_BASE_URL}/v1/annotations",
        json=[annotation],
        headers={
            "Authorization": f"Bearer {API_KEY}",
            "Content-Type": "application/json"
        }
    )
    
    if response.status_code == 200:
        print(f"Feedback submitted successfully for completion {completion_id}")
    else:
        print(f"Error submitting feedback: {response.text}")
    
    return response

# Example usage after getting a completion
completion_id = "anotherai/completion/0198cd7c-2b1a-73b1-ce14-53da0b268569"
accuracy = 4
user_comment = "The response was helpful but could be more concise"
submit_user_feedback(completion_id, accuracy, user_comment)
```
</Step>

<Step>
### Create a custom view to easily review the feedback sent

Ask Claude Code to create a view for you in AnotherAI to see all the feedback in one place.

```
Create a view that shows all completions of anotherai/agent/[your-agent-name] with annotations. 
The view should display the completion ID, input, outputs, and the annotation 
left on the completion.
```

[IMAGE TODO: SCREENSHOT OF VIEW CREATED FROM THIS PROMPT]

</Step>

<Step>
### Using Feedback for Improvements

Ask Claude Code to analyze user feedback and suggest improvements:

```
Review the user feedback annotations for agent/email-writer from the last week 
and suggest prompt improvements based on common complaints
```

Claude will query the annotations, identify patterns, and propose specific changes to improve user satisfaction.

[IMAGE TODO: SCREENSHOTS OF PROMPT IN CLAUDE CODE AND/OR EXPERIEMENT OF AGENT BEFORE AND AFTER FEEDBACK]
</Step>
</Steps>

## Other Ways to Improve your Agents

<Cards>
  <Card title="Using Experiments" href="/use-cases/fundementals/improving_w_experiments" icon={<FlaskRound />}>
    Systematically compare prompts, models, and parameters to find the optimal agent configuration
  </Card>
  <Card title="Using Annotations" href="/use-cases/fundementals/improving_w_annotations" icon={<MessageSquare />}>
    Add specific feedback to completions and experiments to guide agent improvements
  </Card>
</Cards>
