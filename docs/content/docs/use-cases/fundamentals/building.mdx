---
title: Building a New Agent
summary: Learn how to build AI agents with AnotherAI's unified API and observability features
description: Step-by-step guide to creating, configuring, and deploying AI agents using AnotherAI
---

import { Step, Steps } from 'fumadocs-ui/components/steps';
import { Callout } from 'fumadocs-ui/components/callout';
import { Accordion, Accordions } from 'fumadocs-ui/components/accordion';

**Our goal with building with AnotherAI is to turn your AI coding assistant into your AI engineer.** Instead of manually testing different models and prompts, AnotherAI gives your AI coding agent the tools to automatically find the optimal configuration for your specific use case. Here is how to get started building a new agent with AnotherAI.

<Callout type="warning">
Before you begin, make sure you have the AnotherAI MCP server configured with your AI assistant. Your AI assistant needs this connection to create agents, run experiments, and manage deployments. See the [Getting Started guide](/getting-started) for setup instructions.
</Callout>

<Callout type="info">
Need an extra hand with building agents? We're happy to help. Reach us at [team@workflowai.support](mailto:team@workflowai.support) or on [Slack](https://join.slack.com/t/anotherai-dev/shared_invite/zt-3av2prezr-Lz10~8o~rSRQE72m_PyIJA).
</Callout>


## Specifying Agent Behavior and Requirements

The easiest way to create a new agent is to ask Claude Code (or your preferred AI coding agent) to build it for you.

#### Basic Agent Creation

Start by describing what your agent should do:

```
Create a new AnotherAI agent that can summarize emails
```

#### Adding Performance Requirements

If you have other criteria or constraints for your agent, you can include them in your prompt and Claude Code will use AnotherAI to help you optimize for them.

**For customer-facing agents that need fast responses:**

```
Create a new AnotherAI agent that summarizes emails where at least half of the responses complete in under 1 second 
```

**For high-volume agents that need to be cost-effective:**

```
Create a new AnotherAI agent that summarizes emails that costs less than $5 for 1000 requests
```

Claude Code will be able to construct the agent's code, and will be able to use AnotherAI to access 100+ different models to find the best configuration to help you achieve your goals.


If you prefer to build manually or want to understand the configuration details, see our [OpenAI SDK Integration](/docs/integrations/openai) guide.

## Testing your Agent

As part of the process of creating your agent with AnotherAI, Claude Code will automatically create an initial experiment to test your agent's performance. Experiments allow you to systematically compare each of these different parameters of your agent to find the optimal setup for your use case across one or more inputs. You can use experiments to:
- Compare performance across different models (GPT-4, Claude, Gemini, etc.)
- Test multiple prompt variations to find the most effective approach
- Optimize for specific metrics like cost, speed, and accuracy. 

In the cases above where certain constraints were specified, Claude Code will automatically create several versions of your agent to assess which one matches your requirements best.

For the prompt requesting a fast agent:

[IMAGE TODO: SCREENSHOT OF EXPERIMENT SHOWING DIFFERENT FAST MODELS]

For the prompt requesting a cost-effective agent:

[IMAGE TODO: SCREENSHOT OF EXPERIMENT SHOWING DIFFERENT COST-EFFECTIVE MODELS]

If you find there is additional criteria you want to test, you can always ask Claude Code to create additional experiments. The most common parameters to experiment with are prompts and models, however you can also experiment with changes to other parameters like temperature. 

### Prompts

Comparing different prompts is one of the most effective ways to improve your agent's performance. Small changes in wording, structure, or examples can lead to significant improvements. If you notice an issue with an existing prompt, you can even ask Claude to generate prompt variations to use in the experiment. 

Example:

```
Look at the prompt of anotherai/agent/email-rewriter and create an experiment in AnotherAI 
that compares the current prompt with a new prompt that better emphasizes adopting 
the tone lists in the input 
```

[IMAGE TODO: SCREENSHOT OF EXPERIMENT VIEW SHOWING TWO PROMPTS SIDE BY SIDE]

### Models

Different models excel at different tasks. AnotherAI supports over 100 different models, and experiments can help you choose the right model for your agent, depending on its needs.

Example:

```
Create an AnotherAI experiment to help me find a faster model for anotherai/agent/email-rewriter,
but still maintains the same tone and verbosity considerations as my current model.
```

If you have a specific model in mind that you want to try - **for example, a newly released model** - you can ask Claude to help you test that model against your existing agent version. You can always request that Claude use inputs from existing completions, to ensure that you're testing with real, production data. 

Example:

```
Can you retry the last 5 completions of anotherai/agent/email-rewriter and compare the outputs with 
GPT 5 mini?
```

[IMAGE TODO: SCREENSHOT OF EXPERIMENT VIEW SHOWING TWO MODELS SIDE BY SIDE]

### Other Parameters

Beyond prompts and models, fine-tuning other parameters can impact your agent's behavior and output quality. Temperature in particular can have a significant impact on the quality of the output.

<Accordions>
<Accordion title="Temperature">
Temperature is the most important parameter to experiment with after prompts and models. It controls the randomness of the model's output:

- **Low temperature (0.0 - 0.3)**: More deterministic, consistent outputs. Best for:
  - Data extraction tasks
  - Classification
  - Structured output generation
  - Tasks requiring high accuracy and repeatability

- **Medium temperature (0.4 - 0.7)**: Balanced creativity and consistency. Best for:
  - General purpose assistants
  - Question answering
  - Summary generation

- **High temperature (0.8 - 1.0)**: More creative, varied outputs. Best for:
  - Creative writing
  - Brainstorming
  - Generating diverse options

Example:

```
Test my email-rewriter agent with temperatures 0.2, 0.5, and 0.8 to find 
the right balance between creativity and professionalism
```

[IMAGE TODO: SCREENSHOT OF EXPERIMENT VIEW SHOWING 3 versions with different temperatures]
</Accordion>
</Accordions>

## Adding Feedback to your Experiments

When reviewing the results of experiments, you can add feedback (annotations) to help your AI coding agent understand what is working and what is not. Your AI coding agent can then use this feedback to create additional experiments with improved versions of your agent. 

You can add annotations to completions from experiments directly in AnotherAI web app. Annotations can be added for both entire completions and individual fields within the output (when the output is structured).

To add annotations:

<Steps>
<Step>
When your coding agent creates an experiment for you, it will automatically send you a URL to the experiment. Use that URL to open the page for the experiments. 

Or if adding annotations to an experiment later:
- Go to [anotherai.dev/experiments](https://anotherai.dev/experiments)
- Select the experiment you want to add annotations to
</Step>

<Step>
Locate the "Add Annotation" button under each completion's output. Select the button to open a text box where you can add your feedback about the content of that specific completion.
</Step>

<Step>
To annotate individual fields within the output, hover over the field you want to annotate and select the "Add Annotation" button.
</Step>

<Step>
You can also add annotations to the model, prompt, output schema (if structured output is enabled for the agent), and other parameters like temperature, top_p, etc.
</Step>
</Steps>

To learn more about how annotations can be used to improve your agent, see our [Improving an Agent](/docs/agents/improving#annotations) page.

## Next Steps

- [Learn more about different types of experiments](/docs/agents/evaluating)
- [Improve your agent with annotations](/docs/agents/improving)
- [Evaluate agent performance](/docs/agents/evaluating)
