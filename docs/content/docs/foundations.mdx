---
title: Foundations
summary: Essential concepts and architecture of AnotherAI to get you started with agents, models, deployments, and core features.
---

## What is AnotherAI?

Think of AnotherAI as a drop-in replacement for OpenAI API, with primitives added to make it easier to build AI agents.

AnotherAI works with all programming languages, we provide specific examples for Python, Javascript, Typescript, Go, Ruby, Rust, Java, C#.

### Cost 

By using AnotherAI, you won't pay more than your current inference costs. We price match the providers and make our margin through volume discounts. [Learn more about our pricing](/pricing).

## Primitives

### Inference

AnotherAI exposes a compatible OpenAI API endpoint to `/v1/chat/completions`, which means that all SDKs that support OpenAI API will work with AnotherAI by simply changing the base URL and the API key. The list of models supported can be listed by calling the `list_models` MCP tool, or `curl {{API_URL}}/v1/models`.

```python
import openai

client = openai.OpenAI(
    base_url="{{API_URL}}/v1",
    api_key="aai-***", # use create_api_key MCP tool to create an API key
)
```

The primary benefit of using AnotherAI's API is gaining access to a unified interface for all AI models across the market. This eliminates the complexity of managing multiple API keys and switching between different provider SDKs - you can seamlessly use any model through a single, consistent API.

Technical details:
- all requests are proxied by AnotherAI, then sent to an AI provider.

#### API keys

- Check first if any `ANOTHERAI_API_KEY` is set in the environment variables.
- To create a new API key, use the `create_api_key` MCP tool.

### API

You can interact with the API directly, read the OpenAPI spec at {{API_URL}}/openapi.json

### Observability

By default, AnotherAI saves all LLM completions. Observability is critical for building reliable AI agents. LLM observability helps teams improve reliability, reduce costs, debug failures faster, ensure safety, and optimize prompts and models by providing end-to-end visibility into how queries are processed and where issues arise.

#### Viewing Completions in the Web App

To view your completions in the AnotherAI web app, use the `query_completions` MCP tool. This tool validates your SQL query and returns a URL to view the results in the web interface.

Learn more about by reading the [Observability](/observability) section.

### Experiments

Experiments are containers for testing and comparing different model configurations, prompts, and parameters. They enable systematic evaluation and iteration of AI agents.

**Key concepts:**
- Experiments group related completions for analysis
- Each experiment has a unique ID and can be annotated with feedback
- Results are documented for future reference and learning

**Creating experiments with the playground tool:**
The `playground` tool enables systematic testing across multiple dimensions:
- Test multiple models in parallel (e.g., "gpt-4o-mini,claude-3-5-sonnet-20241022")
- Compare different prompt variations and temperatures
- Test with structured inputs using template variables
- Automatically track cost and performance metrics

The tool creates a matrix of completions testing all combinations of models, prompts, inputs, and temperatures.

**Agent workflow:**
1. **Create experiments** using `playground` tool for testing
2. **Add results/conclusions** using `add_experiment_result` with:
   - Performance metrics (speed, cost, accuracy)
   - Key findings and insights
   - Recommendations for next steps
   - Model comparisons and winners
3. **Provide experiment URL** immediately for user review: `{{WEB_APP_URL}}/experiments/{experiment_id}`
4. **Check for user feedback** using `get_annotations`
5. **Iterate** based on feedback

Always add experiment results using `add_experiment_result` after completing any testing or analysis. This tool documents findings, performance metrics, and recommendations for future reference.

**Model selection for experiment creation:**
When creating experiments for agents with complex structured output schemas, asking **Claude Opus** to create the experiment is recommended. Less intelligent models have been known to modify or simplify schemas despite explicit instructions not to. Claude Opus consistently preserves the exact schema specifications as intended.

### Deployments

Deployments allow you to update an agent's prompt or model without changing the code. Learn more about deployments by reading the [Deployments](/deployments) section.

## `/v1/chat/completions`

### Parameters

Building an AI agent is the process of picking the right value for each parameter of the `/v1/chat/completions` API. Let's go through each parameter one by one.

```python
completion = client.chat.completions.create(  # or client.beta.chat.completions.parse for structured outputs
    model="..",
    messages=[...],
    metadata={"agent_id": "...", "key": "value"},
    extra_body={
        "input": {
            "variable_name": "variable_value"
        },
    },
    max_tokens=1000,
)
```

#### model
one of the model.id from the `list_models` MCP tool, or `curl {{API_URL}}/v1/models`
AnotherAI allows non-OpenAI models to be used via a OpenAI SDK.
each model listed by AnotherAI includes information about its price, its intelligence (quality_index), its capabilities, its context window.

#### messages

`messages`: The messages to send to the model. The messages are a list of dictionaries, each dictionary containing a role and content. The role can be "user", "assistant", or "system". The content can be a string, or a list of strings.

```json
messages = [
    {"role": "user", "content": "Hello, how are you?"}
]
```

There are a few differences between the OpenAI API and AnotherAI API:
- **[Input variables](/observability/input-variables) (strongly recommended)**: Use Jinja2 template syntax to separate static instructions from dynamic data. This is a best practice that significantly improves observability, debugging, and prompt management.

```json
messages=[{
    "role": "user", 
    "content": "Analyze this email: {{email_content}}"
}]
```

See the `input` parameter below on how to pass variables to the LLM. Note that the template rendering is done server-side by AnotherAI, so the client does not need to render the template.

Learn more about input variables by reading the [Input Variables](/observability/input-variables) section.

- [Deployments](/deployments): When using deployments, the `messages` parameter can be empty because the messages are stored on AnotherAI directly, and added automatically to the request. `messages = []` is valid. Note that the `messages` parameter is required by OpenAI SDKs, so `messages = None` is not valid.

Learn more about deployments by reading the [Deployments](/deployments) section.

#### metadata

`metadata.agent_id`: (highly recommended) Use a descriptive agent_id to identify the agent in the logs. For example: `email-classifier`, `product-review-sentiment-analyzer`, `customer-support-chatbot`, etc. 

Any key-value pair can be passed to the `metadata` parameter. Runs are searchable by metadata keys (list all the metadata keys for a given agent using the `query_completions` MCP tool). For example, "customer_id": "1234567890", "user_email": "john.doe@example.com".

Learn more about metadata by reading the [Metadata](/observability/metadata) section.

#### max_tokens

`max_tokens`: (optional) The maximum number of tokens to generate. If not provided, the model will generate as many tokens as needed. Make sure that `max_tokens` is high enough to generate a complete response.

#### input

`input`: (strongly recommended for most use cases) provides variables to the LLM when using [input variables](/observability/input-variables). **Always use input variables instead of string concatenation** when you have dynamic content - it dramatically improves debugging, observability, and prompt management.

```python
completion = client.chat.completions.create(
    messages=[{"role": "user", "content": "Analyze this email: {{email_content}}"}],
    extra_body={ # input must be wrapped in extra_body because the OpenAI SDK doesn't recognize 'input' as a valid parameter. extra_body passes custom fields directly to the request body.
        "input": {
            "email_content": "Dear team, please review the quarterly report..."
        }
    }
)
```

**Best practice:** Use input variables for any dynamic content rather than concatenating strings in your code. This separates your prompt logic from your application logic and makes debugging much easier.

#### response_format

`response_format`: (optional) ensures AI models generate responses that perfectly match your defined JSON Schema. Instead of hoping the model follows formatting instructions, you get guaranteed compliance with your data structure. Use structured outputs when you need reliable data extraction, classification, or any scenario requiring consistent JSON format.

```python
from pydantic import BaseModel

class UserInfo(BaseModel):
    name: str
    age: int
    email: str

completion = client.beta.chat.completions.parse(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Extract user info: John Doe, 30, john@example.com"}],
    response_format=UserInfo  # Guarantees valid UserInfo object
)

user = completion.choices[0].message.parsed  # Direct access to typed object
```

Learn more about structured outputs in the [Structured Outputs](/inference/structured-outputs) section.

### Response

AnotherAI returns the same response format as the OpenAI API, ensuring full compatibility with existing code while adding additional fields for enhanced functionality.

```python
completion = client.chat.completions.create(...)
content = completion.choices[0].message.content

# for structured outputs
completion = client.beta.chat.completions.parse(..., response_format=...)
parsed_output = completion.choices[0].message.parsed
```

#### cost and latency

AnotherAI adds cost and latency to the response. Learn more about cost and latency in the [Cost Metadata](/inference/cost) section.

```python
cost = getattr(completion.choices[0], 'cost_usd', None)
latency = getattr(completion.choices[0], 'duration_seconds', None)
print(f"Latency (s): {latency:.2f}")
print(f"Cost   ($): ${cost:.6f}")
```
