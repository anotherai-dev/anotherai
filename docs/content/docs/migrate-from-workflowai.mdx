---
title: Migrating from WorkflowAI to AnotherAI
summary: Guide for migrating agents from WorkflowAI to AnotherAI
---

# Migrating from WorkflowAI to AnotherAI

## Context

WorkflowAI is a layer of abstraction on top of LLM providers, that similarly to AnotherAI allows hitting many providers in the same manner.

The main difference is that in most cases WorkflowAI has a "structured data in structured data out" approach and does not use the message primitive in its basic API.

While not the most common usage, WorkflowAI also offers an OpenAI completion API compatible endpoint. 

> WorkflowAI and AnotherAI share the same authorization servers. So users of WorkflowAI can use the same account on AnotherAI. API Keys are however not transferable. WorkflowAI API keys start with `wai-` and AnotherAI API keys start with `aai-`.


## Migration Steps

Very important: Most WorkflowAI agents use deployments on WorkflowAI, meaning that the code does not contain the prompt. The prompt is stored and managed by the WorkflowAI backend. It is possible to setup a bridge that forwards data from WorkflowAI to AnotherAI which makes the prompt available in AnotherAI.
The most important part of the migration is to **fetch the prompt that is behind the WorkflowAI deployment** and integrate it into the user's code.

The agent ids will be the same in AnotherAI as in WorkflowAI.

### Step 1: Fetch a completion for the agent

If the bridge is setup, you can fetch a completion for the agent using the `query_completions` tool.

```
query_completions(query="SELECT version FROM completions WHERE agent_id = 'your-agent-id' LIMIT 1")
```

> If there is no completion, the bridge is not setup. Advise the user to contact WorkflowAI support.

Once you have a version, you will be able to generate the entire completion call.

### Step 2: Configuring an OpenAI Client

AnotherAI is OpenAI completion API compatible, so it should be configured just like any other OpenAI client. You can use the `create_api_key` tool to create an API key.

> Note that API Keys are usually not stored in code directly, a common pattern is to use environment variables or a .env file

<Tabs items={['Node (OpenAI SDK)', 'API']}>
<Tab>

```typescript
import OpenAI from 'openai';

client = OpenAI(
    base_url="{{API_URL}}/v1/",
    api_key="aai-***",
)
```

</Tab>
<Tab>
```
POST {{API_URL}}/v1/chat/completions
Authorization: Bearer aai-***
```
</Tab>
</Tabs>


### Migrating a WorkflowAI agent that uses the OpenAI completion API compatible endpoint

A WorkflowAI agent that uses the completion API will also have a configured OpenAI client. This client should be updated to point to AnotherAI's base URL and API key.

Example code to be converted:

<Tabs items={['Node (OpenAI SDK)', 'API']}>
<Tab>

```typescript
import OpenAI from 'openai';

client = OpenAI(
    base_url="https://run.workflowai.com/v1/",
    api_key=os.environ["WORKFLOWAI_API_KEY"],
)
```

</Tab>
<Tab>

When hitting the API directly, an http client is usually configured with the base URL and API key.
The exact client depends on the language and libraries that are used.

Simply make sure that:
- the base URL points to `{{API_URL}}`. The full completion URL should look like `{{API_URL}}/v1/chat/completions`
- the Authorization header looks like `Authorization: Bearer aai-***` where `aai-***` is the AnotherAI API key

</Tab>
</Tabs>

You can tell whether or not the agent uses a deployment by checking the model in the OpenAI completion call.

Any model with the format `<agent_id>/#<a number>/<environment, e-g:production|staging|dev>` is a WorkflowAI deployment.

<Tabs items={['Node (OpenAI SDK)', 'API']}>
<Tab>

```typescript
// This is a WorkflowAI deployment
const completion = await client.chat.completions.create({
    model: "travel-assistant/#1/production",
});

// This is not a WorkflowAI deployment
const completion = await client.chat.completions.create({
    model: "gpt-4o",
});
```

</Tab>
<Tab>

Uses deployments:

```sh
POST {{API_URL}}/v1/chat/completions
Authorization: Bearer aai-***
Content-Type: application/json
{
    "model": "travel-assistant/#1/production",
    # messages are optional here
    ...
}
```

Does not use deployments, calls a model directly
```sh
POST {{API_URL}}/v1/chat/completions
Authorization: Bearer aai-***
Content-Type: application/json
{
    "model": "gpt-4o",
    # messages are required here
    ...
}
```
</Tab>
</Tabs>


### Migrating a standard WorkflowAI agent

A standard WorkflowAI agent is an agent that either uses the WorkflowAI SDK or the run endpoint. All WorkflowAI agents use deployments, meaning that the prompt is stored in the WorkflowAI backend and not in the code. When the bridge between WorkflowAI and AnotherAI is setup, you should be able to fetch an imported completion and determine the prompt. The first step in migrating an agent to AnotherAI is to build an OpenAI completion call **without using a deployment**. Deployments will come after the agent has been migrated and the user is happy with the result.

Example code to be converted:

<Tabs items={['Node (WorkflowAI SDK)', 'API']}>
<Tab>

```typescript
// WorkflowAI client setup
// This should be replaced with the OpenAI client setup pointing to AnotherAI
const workflowAI = new WorkflowAI({
  key: process.env["WORKFLOWAI_API_KEY"],
});

// Set up types
// Input type can likely be re-used in a function
export interface AnalyzeBookCharactersTaskInput {
  book_title?: string;
}

// Output type will have to converted to a Zod schema to be compatible with the OpenAI beta SDK
export interface AnalyzeBookCharactersTaskOutput {
  characters?: {
    name?: string;
    goals?: string[];
    weaknesses?: string[];
    outcome?: string;
  }[];
}

// Set up the agent
const analyzeBookCharacters: Agent<
  AnalyzeBookCharactersTaskInput,
  AnalyzeBookCharactersTaskOutput
> = workflowAI.agent({
  id: "analyze-book-characters", // agent id, will be the same in AnotherAI
  schemaId: 1, // schema id, can be ignored, will no longer be used in AnotherAI
  version: "production", // deployment_id, in WorkflowAI deployments are unique per agent schema
});


// Call the agent
const input: AnalyzeBookCharactersTaskInput = {
  book_title: "The Shadow of the Wind",
};
// metadata is optional. most options that could be passed to the function are also available in AnotherAI
const { output, data: { duration_seconds, cost_usd }} = await analyzeBookCharacters(input, { metadata: ...});
// output is of type AnalyzeBookCharactersTaskOutput

```
</Tab>
<Tab>

WorkflowAI exposes a `run` endpoint per agent and schema. The full url will look like `https://workflowai.com/v1/agents/<agent_id>/schemas/<schema_id>/run` where:
- `<agent_id>` is a slug that is the id of the agent
- `<schema_id>` is an integer that identifies a schema (not used in AnotherAI)

The payload will look like:
```json
{
    "version": "production", // deployment_id, in WorkflowAI deployments are unique per agent schema
    "task_input": {
        "book_title": "The Shadow of the Wind"
    },
    "metadata": {
        // metadata is optional
    }
} 
```

</Tab>
</Tabs>

**The only required step is to fetch the version from an Imported completion.**

This can be done using the `query_completions` tool. You will retrieve the entire version that corresponds to the agent.

```
query_completions(query="SELECT version FROM completions WHERE agent_id = 'analyze-book-characters' LIMIT 1")
```

The version payload is a JSON string that contains all the information you need to pass to the OpenAI client.

Example of converted code:

<Tabs items={['Node (OpenAI SDK)', 'API']}>
<Tab>

```typescript
import OpenAI from "openai";
import { zodTextFormat } from "openai/helpers/zod";
import { z } from "zod";


const client = new OpenAI({
  baseURL: "{{API_URL}}/v1",
  apiKey: process.env["ANOTHERAI_API_KEY"],
});

// Typescript here is sufficient
export interface AnalyzeBookCharactersTaskInput {
  book_title?: string;
}

// Output needs to be converted to a Zod schema to be compatible with the OpenAI beta SDK
const AnalyzeBookCharactersTaskOutputSchema = z.object({
  characters: z.array(z.object({
    name: z.string(),
    goals: z.array(z.string()),
    weaknesses: z.array(z.string()),
    outcome: z.string(),
  })),
});
export type AnalyzeBookCharactersTaskOutput = z.infer<typeof AnalyzeBookCharactersTaskOutputSchema>;

export async function analyzeBookCharacters(input: AnalyzeBookCharactersTaskInput): Promise<AnalyzeBookCharactersTaskOutput> {
    const completion = await client.beta.chat.completions.parse({
        model: "<agent_id>/<version.model>", // prefix the model by the agent id
        messages: [
            // use version.prompt here. It will likely contain Jinja2 templates that refer to the input variables.
        ],
        response_format: zodTextFormat(AnalyzeBookCharactersTaskOutputSchema, "AnalyzeBookCharactersTaskOutput"),
        input: input, // input is AnotherAI specific. You may have to silence a TS error
    });
    return completion.output_parsed
}
```

</Tab>
<Tab>

It is likely that an OpenAI SDK exists for the requested language. The OpenAI SDKs are usually more convenient to use, providing out of the box:
- retries / error management
- structured output parsing
- tool calling

It is also possible that the user already has an Agent framework setup that is compatible with the completion API. If re-using an existing Agent framework, it is a good idea to create a separate client as to avoid forcing the user to migrate the entirety of its completion calls.

If hitting the API directly is needed, the payload will look like:
```json
{
    // prefix the model with the agent_id 
    "model": "<agent_id>/<version.model>",
    "messages": [
        // use version.prompt here. It will likely contain Jinja2 templates that refer to the input variables.
    ],
    "response_format": {
        "type": "json_schema",
        // You can use the json_schema from version.output_schema.json_schema
        "json_schema": ...
    },
    "input": ..., // corresponds to task_input in WorkflowAI,
    "metadata": {
        // metadata is optional
    }
}
```

</Tab>
</Tabs>

Once this code is executed once, the agent has properly been migrated to AnotherAI. Good next steps are to allow the user to create experiments via the playground tool and deploy their newly created versions.
